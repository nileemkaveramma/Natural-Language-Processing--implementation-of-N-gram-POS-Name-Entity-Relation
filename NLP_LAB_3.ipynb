{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##NLP Lab - 3"
      ],
      "metadata": {
        "id": "lPZiu_BeT9gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAME: Nileem Kaveramma C C\n",
        "* 2348441"
      ],
      "metadata": {
        "id": "YdQ8oUzTTybl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KqD3DGQy7vif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Estimating N-gram probability\n",
        "* a. Generate a lookup table for all the words in a given text for unigram probability"
      ],
      "metadata": {
        "id": "xHoGU5yJ_zfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A unigram is a single word, and the unigram probability measures the likelihood of that word occurring in a text.\n",
        "* An N-gram is a sequence of N words. The N-gram probability measures the likelihood of a word occurring given the previous\n",
        "N‚àí1 words. The most common N-grams are bigrams (2-grams) and trigrams (3-grams).\n",
        "* Unigram Probability: Probability of individual words occurring (e.g., \"love\" = 0.2).\n",
        "* Bigram Probability: Probability of a word given the previous word (e.g.,\n",
        "ùëÉ\n",
        "(\n",
        "\"natural\"\n",
        "‚à£\n",
        "\"love\"\n",
        ")\n",
        "=\n",
        "1\n",
        "P(\"natural\"‚à£\"love\")=1).\n",
        "* Trigram Probability: Probability of a word given the previous two words (e.g.,\n",
        "ùëÉ\n",
        "(\n",
        "\"language\"‚à£\n",
        "\n",
        "\"love¬†natural\"\n",
        ")\n",
        "=\n",
        "1\n",
        "P(\"language\"‚à£\"love¬†natural\")=1)."
      ],
      "metadata": {
        "id": "YEjiFyzzia8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDWk1n8H_E3P",
        "outputId": "bd5893d2-ed92-4491-e01c-fda40902a956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probabilities Lookup Table:\n",
            "Word: 'the', Probability: 0.0909\n",
            "Word: 'stock', Probability: 0.1818\n",
            "Word: 'market', Probability: 0.0909\n",
            "Word: 'is', Probability: 0.0909\n",
            "Word: 'volatile', Probability: 0.0909\n",
            "Word: 'and', Probability: 0.0909\n",
            "Word: 'prices', Probability: 0.0909\n",
            "Word: 'can', Probability: 0.0909\n",
            "Word: 'change', Probability: 0.0909\n",
            "Word: 'rapidly.', Probability: 0.0909\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Step 1: Example stock market sentence\n",
        "sentence = \"The stock market is volatile and stock prices can change rapidly.\"\n",
        "\n",
        "# Step 2: Tokenize the sentence\n",
        "# Convert to lowercase and split by spaces\n",
        "tokens = sentence.lower().split()\n",
        "\n",
        "# Step 3: Calculate word frequencies\n",
        "word_frequencies = Counter(tokens)\n",
        "\n",
        "# Step 4: Calculate total number of words\n",
        "total_words = len(tokens)\n",
        "\n",
        "# Step 5: Calculate unigram probabilities and create lookup table\n",
        "unigram_probabilities = {word: count / total_words for word, count in word_frequencies.items()}\n",
        "\n",
        "# Display the lookup table\n",
        "print(\"Unigram Probabilities Lookup Table:\")\n",
        "for word, prob in unigram_probabilities.items():\n",
        "    print(f\"Word: '{word}', Probability: {prob:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* b. Train an n-gram language model on a corpus. Compute the perplexity of a test set given by the user.\n",
        "* Example of bigram:\n",
        "P(&lt;s&gt; I want english food &lt;/s&gt;) = ¬† P(I|&lt;s&gt;)"
      ],
      "metadata": {
        "id": "JV20uVbrAAdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Perplexity is a measure of how well a language model predicts a sample."
      ],
      "metadata": {
        "id": "tznzGwfxKU-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the corpus\n",
        "corpus = [\n",
        "    \"<s> I want English food </s>\",\n",
        "    \"<s> I love Indian food </s>\",\n",
        "    \"<s> She wants Italian food </s>\"\n",
        "]\n",
        "\n",
        "# Step 2: Tokenize the corpus\n",
        "tokens = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# Step 3: Train Bigram Model\n",
        "bigram_counts = defaultdict(Counter)\n",
        "unigram_counts = Counter()\n",
        "\n",
        "for sentence in tokens:\n",
        "    for i in range(len(sentence) - 1):\n",
        "        unigram_counts[sentence[i]] += 1\n",
        "        bigram_counts[sentence[i]][sentence[i + 1]] += 1\n",
        "    # Include the last word unigram count\n",
        "    unigram_counts[sentence[-1]] += 1\n",
        "\n",
        "# Step 4: Calculate Bigram Probabilities\n",
        "bigram_probabilities = defaultdict(dict)\n",
        "\n",
        "for w1 in bigram_counts:\n",
        "    total_count_w1 = float(sum(bigram_counts[w1].values()))\n",
        "    for w2 in bigram_counts[w1]:\n",
        "        bigram_probabilities[w1][w2] = bigram_counts[w1][w2] / total_count_w1\n",
        "\n",
        "# Display Bigram Probabilities\n",
        "print(\"Bigram Probabilities:\")\n",
        "for w1 in bigram_probabilities:\n",
        "    for w2 in bigram_probabilities[w1]:\n",
        "        print(f\"P({w2}|{w1}) = {bigram_probabilities[w1][w2]:.4f}\")\n",
        "\n",
        "# Step 5: Compute Perplexity on a test sentence\n",
        "def compute_sentence_probability(sentence, bigram_probabilities):\n",
        "    words = sentence.split()\n",
        "    probability = 1.0\n",
        "    for i in range(len(words) - 1):\n",
        "        w1 = words[i]\n",
        "        w2 = words[i + 1]\n",
        "        prob = bigram_probabilities[w1].get(w2, 1e-6)  # Use a small smoothing value for unseen bigrams\n",
        "        probability *= prob\n",
        "    return probability\n",
        "\n",
        "def compute_perplexity(test_sentence, bigram_probabilities):\n",
        "    words = test_sentence.split()\n",
        "    sentence_probability = compute_sentence_probability(test_sentence, bigram_probabilities)\n",
        "    N = len(words)\n",
        "    perplexity = np.power(1 / sentence_probability, 1 / N)\n",
        "    return perplexity\n",
        "\n",
        "# Example test sentence\n",
        "test_sentence = \"<s> I want Italian food </s>\"\n",
        "\n",
        "# Compute and print perplexity\n",
        "perplexity = compute_perplexity(test_sentence, bigram_probabilities)\n",
        "print(f\"Perplexity of the test sentence: {perplexity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5yDjRV7AHWf",
        "outputId": "059b735a-d1b8-45e9-ff02-b0db991fd4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probabilities:\n",
            "P(I|<s>) = 0.6667\n",
            "P(She|<s>) = 0.3333\n",
            "P(want|I) = 0.5000\n",
            "P(love|I) = 0.5000\n",
            "P(English|want) = 1.0000\n",
            "P(food|English) = 1.0000\n",
            "P(</s>|food) = 1.0000\n",
            "P(Indian|love) = 1.0000\n",
            "P(food|Indian) = 1.0000\n",
            "P(wants|She) = 1.0000\n",
            "P(Italian|wants) = 1.0000\n",
            "P(food|Italian) = 1.0000\n",
            "Perplexity of the test sentence: 12.0094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. POS Tagging\n",
        "* a. Implement any one exercise from chapter 8 of Text book to tag the word of the input text."
      ],
      "metadata": {
        "id": "Gkbyh2ihAbq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Step 1: Download NLTK resources (if not already downloaded)\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 2: Prepare the corpus data\n",
        "train_data = treebank.tagged_sents()  # Get tagged sentences from the Treebank corpus\n",
        "\n",
        "# Step 3: Train a Unigram Tagger\n",
        "unigram_tagger = UnigramTagger(train_data)\n",
        "\n",
        "# Step 4: Define a function to tag an input text\n",
        "def pos_tag_text(text, tagger):\n",
        "    # Tokenize the input text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Use the tagger to assign POS tags\n",
        "    tagged = tagger.tag(tokens)\n",
        "\n",
        "    return tagged\n",
        "\n",
        "# Step 5: Example input text\n",
        "input_text = \"The stock market is extremely volatile today.\"\n",
        "\n",
        "# Step 6: POS tag the input text using the trained unigram tagger\n",
        "tagged_text = pos_tag_text(input_text, unigram_tagger)\n",
        "\n",
        "# Display the tagged text\n",
        "print(\"POS Tagged Text:\")\n",
        "print(tagged_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TeX6HinAdsT",
        "outputId": "ddd4622e-29b8-4a6f-9b31-cb29279d784f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagged Text:\n",
            "[('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('extremely', 'RB'), ('volatile', 'JJ'), ('today', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE:**\n",
        "* Word: \"The\" -\n",
        "POS Tag: DT (Determiner)\n",
        "* Word: \"stock\" -\n",
        "POS Tag: NN (Noun, singular or mass)\n",
        "* Word: \"market\" -\n",
        "POS Tag: NN (Noun, singular or mass)\n",
        "* Word: \"is\" -\n",
        "POS Tag: VBZ (Verb, 3rd person singular present)\n",
        "* Word: \"extremely\" -\n",
        "POS Tag: RB (Adverb)\n",
        "* Word: \"volatile\"\n",
        "POS Tag: JJ (Adjective)\n",
        "* Word: \"today\" -\n",
        "POS Tag: NN (Noun, singular or mass)\n",
        "* Word: \".\" -\n",
        "POS Tag: . (Punctuation mark)"
      ],
      "metadata": {
        "id": "adUr4ucV5WpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Generate HMM tagger using transition and observational probabilities."
      ],
      "metadata": {
        "id": "NGwOGej2ATzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* An HMM tagger (Hidden Markov Model tagger) is a type of probabilistic model used in Natural Language Processing (NLP) for Part-of-Speech (POS) tagging. It uses a Hidden Markov Model (HMM) to assign the most likely sequence of POS tags to a given sequence of words in a sentence."
      ],
      "metadata": {
        "id": "rpmx6e3N6XMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Download NLTK resources (if not already downloaded)\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Prepare the corpus data\n",
        "train_data = treebank.tagged_sents()\n",
        "\n",
        "# Initialize counters for transition and emission probabilities\n",
        "tag_transitions = defaultdict(Counter)\n",
        "tag_emissions = defaultdict(Counter)\n",
        "tag_counts = Counter()\n",
        "\n",
        "# Populate the counts\n",
        "for sentence in train_data:\n",
        "    prev_tag = None\n",
        "    for word, tag in sentence:\n",
        "        tag_counts[tag] += 1\n",
        "        tag_emissions[tag][word] += 1\n",
        "        if prev_tag is not None:\n",
        "            tag_transitions[prev_tag][tag] += 1\n",
        "        prev_tag = tag\n",
        "\n",
        "# Calculate transition probabilities\n",
        "tag_transition_probs = defaultdict(dict)\n",
        "for tag1, tags2 in tag_transitions.items():\n",
        "    total = float(sum(tags2.values()))\n",
        "    for tag2, count in tags2.items():\n",
        "        tag_transition_probs[tag1][tag2] = count / total\n",
        "\n",
        "# Calculate emission probabilities\n",
        "tag_emission_probs = defaultdict(dict)\n",
        "for tag, words in tag_emissions.items():\n",
        "    total = float(tag_counts[tag])\n",
        "    for word, count in words.items():\n",
        "        tag_emission_probs[tag][word] = count / total\n",
        "\n",
        "# Viterbi Algorithm for HMM Tagging\n",
        "def viterbi(sentence, tags):\n",
        "    n = len(sentence)\n",
        "    dp = [{} for _ in range(n)]\n",
        "    backpointer = [{} for _ in range(n)]\n",
        "\n",
        "    # Initialize base cases\n",
        "    for tag in tags:\n",
        "        dp[0][tag] = tag_emission_probs[tag].get(sentence[0], 1e-6)\n",
        "        backpointer[0][tag] = None\n",
        "\n",
        "    # Dynamic programming to fill the dp table\n",
        "    for i in range(1, n):\n",
        "        for tag in tags:\n",
        "            max_prob, best_tag = max(\n",
        "                ((dp[i-1][prev_tag] * tag_transition_probs[prev_tag].get(tag, 1e-6) *\n",
        "                  tag_emission_probs[tag].get(sentence[i], 1e-6), prev_tag)\n",
        "                 for prev_tag in tags),\n",
        "                key=lambda x: x[0]\n",
        "            )\n",
        "            dp[i][tag] = max_prob\n",
        "            backpointer[i][tag] = best_tag\n",
        "\n",
        "    # Backtrack to get the best sequence of tags\n",
        "    best_sequence = []\n",
        "    best_last_tag = max(dp[-1], key=dp[-1].get)\n",
        "    best_sequence.append(best_last_tag)\n",
        "    for i in range(n-1, 0, -1):\n",
        "        best_last_tag = backpointer[i][best_last_tag]\n",
        "        best_sequence.append(best_last_tag)\n",
        "    best_sequence.reverse()\n",
        "\n",
        "    return best_sequence\n",
        "\n",
        "# Define a list of possible tags (based on the training data)\n",
        "tags = list(tag_counts.keys())\n",
        "\n",
        "# Example input text\n",
        "input_text = \"The stock market is extremely volatile today.\"\n",
        "tokens = nltk.word_tokenize(input_text)\n",
        "\n",
        "# Tag the input text\n",
        "tagged_sequence = viterbi(tokens, tags)\n",
        "\n",
        "# Display the tagged text\n",
        "print(\"HMM Tagged Text:\")\n",
        "for word, tag in zip(tokens, tagged_sequence):\n",
        "    print(f\"{word}: {tag}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GqXENkpRVOg",
        "outputId": "848a8bce-17ca-4217-9c6e-c42dd8beb2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM Tagged Text:\n",
            "The: DT\n",
            "stock: NN\n",
            "market: NN\n",
            "is: VBZ\n",
            "extremely: RB\n",
            "volatile: JJ\n",
            "today: NN\n",
            ".: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement NER with NLTK/spaCY/Stanford NER Tagger\n",
        "\n",
        "Input: John ¬† lives in New ¬† York\n",
        "Output: B-PER¬† O ¬† ¬† O¬† B-LOC I-LOC"
      ],
      "metadata": {
        "id": "uIOPeFIxR1wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model for English\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"John lives in New York\"\n",
        "\n",
        "# Process the sentence with the NLP model\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Initialize an empty list to store the output\n",
        "output = []\n",
        "\n",
        "# Loop through the tokens in the sentence\n",
        "for token in doc:\n",
        "    if token.ent_iob_ == 'O':\n",
        "        output.append(token.ent_iob_)\n",
        "    else:\n",
        "        output.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
        "\n",
        "# Print the formatted output\n",
        "print(\" \".join(output))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WofgPterRYdu",
        "outputId": "3d8a092e-a70b-41f7-8bef-f9b7d172047f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B-PERSON O O B-GPE I-GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE**\n",
        "* BIO Format: The BIO format is a common tagging scheme for NER that stands for:\n",
        "* B-PER: \"B\" denotes the beginning of a named entity of type \"PER\" (person).\n",
        "* I-LOC: \"I\" denotes that the word is inside a named entity of type \"LOC\" (location).\n",
        "* O: \"O\" denotes a token that is outside any named entity."
      ],
      "metadata": {
        "id": "vRUA0bdOFZdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given the example input \"John lives in New York\", the output in BIO format is:\n",
        "\n",
        "* \"John\" ‚Üí B-PER (Beginning of a person entity)\n",
        "* \"lives\" ‚Üí O (Outside any entity)\n",
        "* \"in\" ‚Üí O (Outside any entity)\n",
        "* \"New\" ‚Üí B-LOC (Beginning of a location entity)\n",
        "* \"York\" ‚Üí I-LOC (Inside the same location entity)\n"
      ],
      "metadata": {
        "id": "zW146Op9F4Eg"
      }
    }
  ]
}